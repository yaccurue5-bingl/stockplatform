import os
import json
import logging
import time
from datetime import datetime
from groq import Groq
from supabase import create_client, Client

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

if not GROQ_API_KEY:
    logger.error("âŒ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GitHub Secretsë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
groq_client = Groq(api_key=GROQ_API_KEY)

class AIAnalyst:
    def __init__(self):
        self.system_prompt = """
You are an expert Korean stock market analyst. Analyze the disclosure and respond ONLY with valid JSON.

Required JSON format:
{
  "headline": "Brief English headline (max 10 words)",
  "summary": ["Bullet point 1", "Bullet point 2", "Bullet point 3"],
  "sentiment": "POSITIVE or NEGATIVE or NEUTRAL",
  "sentiment_score": 0.75,
  "importance": "HIGH or MEDIUM or LOW"
}

Rules:
- sentiment_score: -1.0 (very negative) to 1.0 (very positive)
- POSITIVE: good news (earnings up, partnerships, expansion)
- NEGATIVE: bad news (losses, lawsuits, recalls)
- NEUTRAL: routine filings
- HIGH importance: M&A, major contracts, earnings surprises
- MEDIUM: regular earnings, minor partnerships
- LOW: procedural filings, routine updates
        """

    def analyze_content(self, corp_name, title):
        try:
            response = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile", 
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": f"Company: {corp_name}\nDisclosure Title: {title}"}
                ],
                response_format={"type": "json_object"},
                temperature=0.3  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì‚¬ìš©
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # âœ… í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ["sentiment", "sentiment_score", "importance", "summary"]
            for field in required_fields:
                if field not in result:
                    logger.warning(f"âš ï¸ Missing field '{field}' in Groq response, using default")
                    if field == "sentiment":
                        result[field] = "NEUTRAL"
                    elif field == "sentiment_score":
                        result[field] = 0.0
                    elif field == "importance":
                        result[field] = "MEDIUM"
                    elif field == "summary":
                        result[field] = ["ë¶„ì„ ì •ë³´ ì—†ìŒ"]
            
            # âœ… sentiment_score ë²”ìœ„ ê²€ì¦ (-1.0 ~ 1.0)
            score = float(result.get("sentiment_score", 0.0))
            result["sentiment_score"] = max(-1.0, min(1.0, score))
            
            # âœ… sentiment ëŒ€ë¬¸ì ë³€í™˜
            result["sentiment"] = result.get("sentiment", "NEUTRAL").upper()
            
            # âœ… importance ëŒ€ë¬¸ì ë³€í™˜
            result["importance"] = result.get("importance", "MEDIUM").upper()
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON Parse Error: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Groq Analysis Error: {e}")
            return None

def run():
    analyst = AIAnalyst()
    
    # âœ… ai_summaryê°€ NULLì¸ í•­ëª©ë§Œ ê°€ì ¸ì˜¤ê¸° (dart_crawlerê°€ NULLë¡œ ì €ì¥í•˜ë¯€ë¡œ ì‘ë™í•¨)
    res = supabase.table("disclosure_insights") \
        .select("*") \
        .is_("ai_summary", "null") \
        .order("created_at", desc=True) \
        .limit(20) \
        .execute()
    
    if not res.data:
        logger.info("âœ… ë¶„ì„í•  ìƒˆë¡œìš´ ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸ” {len(res.data)}ê±´ì˜ ê³µì‹œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    
    success_count = 0
    fail_count = 0
    
    for item in res.data:
        logger.info(f"ğŸ“Š ë¶„ì„ ì¤‘: {item['corp_name']} - {item['report_nm'][:50]}...")
        
        # Groq API í˜¸ì¶œ
        result = analyst.analyze_content(item['corp_name'], item['report_nm'])
        
        if result:
            # âœ… summary ë¦¬ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
            summary_text = "\n".join(result.get("summary", []))
            
            update_data = {
                "ai_summary": summary_text,
                "sentiment": result.get("sentiment", "NEUTRAL"),
                "sentiment_score": result.get("sentiment_score", 0.0),
                "importance": result.get("importance", "MEDIUM"),
                "updated_at": datetime.now().isoformat()
            }
            
            # DB ì—…ë°ì´íŠ¸
            try:
                supabase.table("disclosure_insights").update(update_data).eq("id", item['id']).execute()
                logger.info(f"âœ… ë¶„ì„ ì„±ê³µ: {item['corp_name']} | Sentiment: {update_data['sentiment']} ({update_data['sentiment_score']:.2f}) | Importance: {update_data['importance']}")
                success_count += 1
            except Exception as e:
                logger.error(f"âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ID: {item['id']}): {e}")
                fail_count += 1
        else:
            logger.warning(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {item['corp_name']}")
            fail_count += 1
        
        # Groq API ì†ë„ ì œí•œ ë°©ì§€ (ë¶„ë‹¹ 30íšŒ ì œí•œ ëŒ€ë¹„)
        time.sleep(2.5)
    
    logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ - ì„±ê³µ: {success_count}ê±´, ì‹¤íŒ¨: {fail_count}ê±´")

if __name__ == "__main__":
    run()