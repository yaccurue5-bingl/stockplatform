import os
import json
import logging
import time
import hashlib
from datetime import datetime
from groq import Groq
from supabase import create_client, Client
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 1. ë¡œì»¬ ê²½ë¡œ ì„¤ì •
local_env_path = r"C:\stockplatform\.env.local"

# 2. ë¡œì»¬ ê²½ë¡œì— íŒŒì¼ì´ ìˆì„ ë•Œë§Œ ë¡œë“œ (Windows ë¡œì»¬ í™˜ê²½)
if os.path.exists(local_env_path):
    load_dotenv(local_env_path)
    logger.info(f"Loaded config from {local_env_path}")
else:
    # 3. ë¡œì»¬ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜(ì˜¨ë¼ì¸ ì„œë¹„ìŠ¤ ì„¤ì •ê°’)ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©
    load_dotenv() 
    logger.info("Using system environment variables")

def generate_hash_key(corp_code: str, rcept_no: str) -> str:
    """ê³µì‹œ hash key ìƒì„±"""
    return hashlib.sha256(f"{corp_code}_{rcept_no}".encode()).hexdigest()

SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

if not GROQ_API_KEY:
    logger.error("âŒ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
groq_client = Groq(api_key=GROQ_API_KEY)

class AIAnalyst:
    def __init__(self):
        self.system_prompt = """
You are a Korean stock market analyst. Analyze disclosures and respond with JSON.

Required format:
{
  "headline": "Brief English headline (max 10 words)",
  "summary": ["Bullet 1", "Bullet 2", "Bullet 3"],
  "sentiment": "POSITIVE or NEGATIVE or NEUTRAL",
  "sentiment_score": 0.75,
  "importance": "HIGH or MEDIUM or LOW"
}

Scoring rules:
- sentiment_score: -1.0 (very negative) to 1.0 (very positive)
- POSITIVE: good news (profits up, partnerships, expansion)
- NEGATIVE: bad news (losses, lawsuits, recalls)
- NEUTRAL: routine filings
- HIGH: M&A, major contracts, large earnings changes
- MEDIUM: regular earnings, minor partnerships
- LOW: procedural filings
"""

    def analyze_content(self, corp_name, title):
        try:
            response = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": f"Company: {corp_name}\nDisclosure: {title}"}
                ],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_completion_tokens=1000 # í† í° ì œí•œ ì¶”ê°€
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
            if "sentiment" not in result:
                result["sentiment"] = "NEUTRAL"
            if "sentiment_score" not in result:
                result["sentiment_score"] = 0.0
            if "importance" not in result:
                result["importance"] = "MEDIUM"
            if "summary" not in result or not isinstance(result["summary"], list):
                result["summary"] = ["ë¶„ì„ ë°ì´í„° ìƒì„± ì¤‘"]
            
            # ê°’ ì •ê·œí™”
            result["sentiment_score"] = max(-1.0, min(1.0, float(result["sentiment_score"])))
            result["sentiment"] = result["sentiment"].upper()
            result["importance"] = result["importance"].upper()
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Groq API Error: {e}")
            return None

def run():
    analyst = AIAnalyst()
    
    # âœ… pending ìƒíƒœì´ê³  ì¬ì‹œë„ íšŸìˆ˜ê°€ 3íšŒ ë¯¸ë§Œì¸ í•­ëª©ë§Œ ì¡°íšŒ
    res = supabase.table("disclosure_insights") \
        .select("*") \
        .eq("analysis_status", "pending") \
        .or_("analysis_retry_count.is.null,analysis_retry_count.lt.3") \
        .order("created_at", desc=True) \
        .limit(200) \
        .execute()
    
    if not res.data:
        logger.info("âœ… ë¶„ì„í•  ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸ” {len(res.data)}ê±´ ë¶„ì„ ì‹œì‘...")
    
    success_count = 0
    fail_count = 0
    
    for item in res.data:
        # ğŸ“„ ë¶„ì„ ì‹œì‘ - ìƒíƒœë¥¼ 'processing'ìœ¼ë¡œ ë³€ê²½
        try:
            supabase.table("disclosure_insights") \
                .update({
                    "analysis_status": "processing",
                    "updated_at": datetime.now().isoformat()
                }) \
                .eq("id", item['id']) \
                .execute()
        except Exception as e:
            logger.error(f"âŒ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            continue
        
        logger.info(f"ğŸ“Š ë¶„ì„ ì¤‘: {item['corp_name']} - {item['report_nm'][:50]}...")
        
        result = analyst.analyze_content(item['corp_name'], item['report_nm'])
        
        if result:
            summary_text = "\n".join(result.get("summary", []))

            update_data = {
                "ai_summary": summary_text,
                "sentiment": result.get("sentiment", "NEUTRAL"),
                "sentiment_score": result.get("sentiment_score", 0.0),
                "importance": result.get("importance", "MEDIUM"),
                "analysis_status": "completed",  # âœ… ë¶„ì„ ì™„ë£Œ
                "updated_at": datetime.now().isoformat()
            }

            try:
                # disclosure_insights ì—…ë°ì´íŠ¸
                supabase.table("disclosure_insights") \
                    .update(update_data) \
                    .eq("id", item['id']) \
                    .execute()

                # âœ… disclosure_hashesì— Groq ë¶„ì„ ì™„ë£Œ ê¸°ë¡
                try:
                    corp_code = item.get('corp_code', '')
                    rcept_no = item.get('rcept_no', '')

                    if corp_code and rcept_no:
                        hash_key = generate_hash_key(corp_code, rcept_no)
                        supabase.table("disclosure_hashes") \
                            .update({
                                "groq_analyzed": True,
                                "groq_analyzed_at": datetime.now().isoformat()
                            }) \
                            .eq("hash_key", hash_key) \
                            .execute()
                        logger.debug(f"âœ… Hash ì—…ë°ì´íŠ¸: {hash_key[:16]}...")
                except Exception as hash_err:
                    logger.warning(f"âš ï¸ Hash ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¶„ì„ì€ ì™„ë£Œ): {hash_err}")

                success_count += 1
                logger.info(f"âœ… ì™„ë£Œ: {item['corp_name']} | {update_data['sentiment']} ({update_data['sentiment_score']:.2f}) | {update_data['importance']}")
            except Exception as e:
                fail_count += 1
                logger.error(f"âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ìƒíƒœë¥¼ 'failed'ë¡œ ë³€ê²½
                try:
                    retry_count = item.get('analysis_retry_count', 0) + 1
                    supabase.table("disclosure_insights") \
                        .update({
                            "analysis_status": "failed",
                            "analysis_retry_count": retry_count,
                            "updated_at": datetime.now().isoformat()
                        }) \
                        .eq("id", item['id']) \
                        .execute()
                except:
                    pass
        else:
            fail_count += 1
            logger.warning(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {item['corp_name']}")
            # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            try:
                retry_count = item.get('analysis_retry_count', 0) + 1
                # 3íšŒ ì´ìƒ ì‹¤íŒ¨ ì‹œ failed, ê·¸ ì´í•˜ë©´ pendingìœ¼ë¡œ ë³µì›
                new_status = "failed" if retry_count >= 3 else "pending"
                
                supabase.table("disclosure_insights") \
                    .update({
                        "analysis_status": new_status,
                        "analysis_retry_count": retry_count,
                        "updated_at": datetime.now().isoformat()
                    }) \
                    .eq("id", item['id']) \
                    .execute()
                
                logger.info(f"   ì¬ì‹œë„ íšŸìˆ˜: {retry_count}/3 | ìƒíƒœ: {new_status}")
            except:
                pass
        
        # API ì†ë„ ì œí•œ ë°©ì§€
        time.sleep(2.5)
    
    logger.info(f"\n{'='*70}")
    logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ")
    logger.info(f"   - ì„±ê³µ: {success_count}ê±´")
    logger.info(f"   - ì‹¤íŒ¨: {fail_count}ê±´")
    logger.info(f"{'='*70}\n")

if __name__ == "__main__":
    run()