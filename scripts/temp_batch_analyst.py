import os
import json
import logging
import time
from datetime import datetime
from groq import Groq
from supabase import create_client, Client
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •: íŒŒì¼ë¡œë„ ì €ì¥í•˜ì—¬ ë©°ì¹ ê°„ì˜ ì§„í–‰ ìƒí™© ì¶”ì  ê°€ëŠ¥
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("batch_analysis.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(r"C:\stockplatform\.env.local")

SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
groq_client = Groq(api_key=GROQ_API_KEY)

class BatchAnalyst:
    def __init__(self):
        # ê¸°ì¡´ auto_analyst.pyì˜ í”„ë¡¬í”„íŠ¸ì™€ ë¡œì§ì„ ê³„ìŠ¹
        self.system_prompt = """
You are a professional Korean stock analyst. 
Analyze the provided disclosure content to determine its financial impact.
Respond ONLY in JSON format.
"""

    def analyze(self, corp_name, report_nm, content):
        try:
            # ë³¸ë¬¸ ê°€ìš©ì„± ì²´í¬ ë¡œì§
            is_invalid = not content or content == "CONTENT_NOT_AVAILABLE" or len(str(content)) < 20
            input_text = f"Title: {report_nm}\n(Analyze based on title)" if is_invalid else f"Title: {report_nm}\nContent: {content}"

            response = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": f"Company: {corp_name}\n{input_text}"}
                ],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            logger.error(f"âŒ {corp_name} ë¶„ì„ ì—ëŸ¬: {e}")
            return None

def run_batch():
    analyst = BatchAnalyst()
    MAX_DAILY_LIMIT = 1000  # í•˜ë£¨ ìµœëŒ€ ë¶„ì„ ê±´ìˆ˜
    processed_count = 0
    
    logger.info("ğŸš€ ì„ì‹œ ë°°ì¹˜ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ê°€ë™ (ì¼ì¼ í•œë„: 1000ê±´)")

    while processed_count < MAX_DAILY_LIMIT:
        # ì•„ì§ 'completed'ê°€ ì•„ë‹ˆê±°ë‚˜, ì˜ˆì „ì— ì œëª©ë§Œìœ¼ë¡œ ë¶„ì„í–ˆë˜ ë°ì´í„°(pending)ë¥¼ 1ê±´ì”© ê°€ì ¸ì˜´
        # 'content'ê°€ ì±„ì›Œì§„ ë°ì´í„°ë¶€í„° ìš°ì„  ì²˜ë¦¬
        res = supabase.table("disclosure_insights") \
            .select("id, corp_name, report_nm, content") \
            .eq("analysis_status", "pending") \
            .is_not("content", "null") \
            .limit(1) \
            .execute()

        if not res.data:
            logger.info("âœ… ëª¨ë“  ëŒ€ìƒ ë°ì´í„°ì˜ ì¬ê°€ê³µì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            break

        item = res.data[0]
        logger.info(f"ğŸ”„ [{processed_count + 1}/{MAX_DAILY_LIMIT}] {item['corp_name']} ë¶„ì„ ì¤‘...")

        result = analyst.analyze(item['corp_name'], item['report_nm'], item['content'])

        if result:
            update_data = {
                "ai_summary": "\n".join(result.get("summary", ["ë‚´ìš© ì—†ìŒ"])),
                "sentiment": result.get("sentiment", "NEUTRAL").upper(),
                "sentiment_score": float(result.get("sentiment_score", 0.0)),
                "importance": result.get("importance", "MEDIUM").upper(),
                "analysis_status": "completed", # ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
                "updated_at": datetime.now().isoformat()
            }
            supabase.table("disclosure_insights").update(update_data).eq("id", item['id']).execute()
            processed_count += 1
            
            # ğŸ’¡ 1ë¶„ë‹¹ 1ê°œ ë¶„ì„ ì œí•œì„ ìœ„í•œ ëŒ€ê¸° ì‹œê°„ (60ì´ˆ)
            logger.info("ğŸ˜´ ë‹¤ìŒ ë¶„ì„ì„ ìœ„í•´ 60ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(60)
        else:
            # ì‹¤íŒ¨ ì‹œ ì ì‹œ í›„ ì¬ì‹œë„í•  ìˆ˜ ìˆê²Œ statusë¥¼ ìœ ì§€í•˜ê±°ë‚˜ retry_count ì¦ê°€
            time.sleep(10)

    logger.info(f"ğŸ ì˜¤ëŠ˜ì˜ í• ë‹¹ëŸ‰({processed_count}ê±´)ì„ ëª¨ë‘ ë§ˆì³¤ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_batch()