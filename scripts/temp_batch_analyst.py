import os
import json
import logging
import time
import sys
from datetime import datetime
from groq import Groq
from supabase import create_client, Client
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì • (UTF-8 ì¸ì½”ë”© ì ìš©)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("batch_test.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(r"C:\stockplatform\.env.local")

supabase: Client = create_client(os.environ.get("NEXT_PUBLIC_SUPABASE_URL"), os.environ.get("SUPABASE_SERVICE_ROLE_KEY"))
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

# auto_analyst.pyì˜ AIAnalyst í´ë˜ìŠ¤ ì„í¬íŠ¸
from auto_analyst import AIAnalyst

def run_test():
    analyst = AIAnalyst()
    processed_count = 0  # âœ… ë³€ìˆ˜ ì„ ì–¸ ìœ„ì¹˜ í™•ì¸
    
    # 1. ë¶„ì„ ëŒ€ìƒ 100ê°œ ì¶”ì¶œ
    res = supabase.table("disclosure_insights") \
        .select("id, corp_name, report_nm, content") \
        .eq("analysis_status", "pending") \
        .not_.is_("content", "null") \
        .limit(500) \
        .execute()

    if not res.data:
        logger.info("âœ… ë¶„ì„í•  ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸš€ ì´ {len(res.data)}ê°œì˜ ê³µì‹œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # 2. ê°€ì ¸ì˜¨ ë°ì´í„°ì— ëŒ€í•´ ë£¨í”„ ì‹¤í–‰ (ë¶„ì„ ë¡œì§ì„ ë£¨í”„ ì•ˆìœ¼ë¡œ ì´ë™)
    for item in res.data:
        try:
            logger.info(f"ğŸ”„ ë¶„ì„ ì¤‘: {item['corp_name']} - {item['report_nm']}")
            
            # ì‹¤ì œ AI ë¶„ì„ í˜¸ì¶œ
            result = analyst.analyze_content(item['corp_name'], item['report_nm'], item['content'])

            if result:
                # âœ… ìµœì‹  í”„ë¡¬í”„íŠ¸ êµ¬ì¡°(key_numbers + ai_summary)ì— ë§ì¶° ë‚´ìš© ìƒì„±
                key_numbers = "\n".join(result.get("key_numbers", []))
                analysis = result.get("ai_summary", "")
                combined_summary = f"[Key Numbers]\n{key_numbers}\n\n[Investment Analysis]\n{analysis}"

                update_data = {
                    "report_nm": result.get("report_nm"),
                    "ai_summary": combined_summary,
                    "sentiment": result.get("financial_impact", "NEUTRAL").upper(),
                    "importance": "HIGH" if int(result.get("short_term_impact_score", 0)) >= 4 else "MEDIUM",
                    "analysis_status": "completed",
                    "updated_at": datetime.now().isoformat(),
                    "headline": result.get("headline", "")
                }
                
                # DB ì—…ë°ì´íŠ¸
                supabase.table("disclosure_insights").update(update_data).eq("id", item['id']).execute()
                
                processed_count += 1
                logger.info(f"âœ… [{processed_count}/{len(res.data)}] ì™„ë£Œ: {item['corp_name']}")
                
                # Groq API ì†ë„ ì œí•œ(Rate Limit) ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5) 
                
            else:
                logger.warning(f"âš ï¸ {item['corp_name']} ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                time.sleep(1)

        except Exception as e:
            logger.error(f"âŒ {item['corp_name']} ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            continue

    logger.info(f"ğŸ ì´ {processed_count}ê°œì˜ ë¶„ì„ì´ ì™„ë£Œë˜ì–´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    run_test()