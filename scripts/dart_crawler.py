import os
import requests
from datetime import datetime, timedelta
from supabase import create_client, Client
import urllib3
import logging
import hashlib

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

url = "https://rxcwqsolfrjhomeusyza.supabase.co"
key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
supabase: Client = create_client(url, key)

def generate_hash_key(corp_code: str, rcept_no: str) -> str:
    """ê³µì‹œ hash key ìƒì„±"""
    return hashlib.sha256(f"{corp_code}_{rcept_no}".encode()).hexdigest()

def is_disclosure_processed(corp_code: str, rcept_no: str) -> bool:
    """ì´ë¯¸ ì²˜ë¦¬ëœ ê³µì‹œì¸ì§€ í™•ì¸"""
    try:
        hash_key = generate_hash_key(corp_code, rcept_no)
        result = supabase.table("disclosure_hashes") \
            .select("id") \
            .eq("hash_key", hash_key) \
            .gt("expires_at", datetime.now().isoformat()) \
            .execute()

        return len(result.data) > 0
    except Exception as e:
        logger.warning(f"í•´ì‹œ í™•ì¸ ì‹¤íŒ¨ (ì²˜ë¦¬ ì§„í–‰): {e}")
        return False

def run_crawler():
    today = datetime.now().strftime('%Y%m%d')
    dart_key = os.environ.get("DART_API_KEY")
    api_url = f"https://opendart.fss.or.kr/api/list.json?crtfc_key={dart_key}&bgnde={today}&endde={today}&page_count=100"

    logger.info(f"ğŸ“¡ DART ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {today}")

    try:
        # SSL/TLS HandshakeFailure í•´ê²°: verify=False ì‚¬ìš©
        res = requests.get(api_url, verify=False, timeout=30)
        res.raise_for_status()
        data = res.json()
    except requests.exceptions.SSLError as ssl_err:
        logger.error(f"âŒ SSL/TLS ì—ëŸ¬: {ssl_err}")
        logger.info("ğŸ’¡ verify=Falseë¡œ ì¬ì‹œë„ ì¤‘...")
        res = requests.get(api_url, verify=False, timeout=30)
        data = res.json()
    except Exception as e:
        logger.error(f"âŒ DART API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return

    if data.get("status") == "000":
        count = 0
        skipped = 0
        duplicates = 0

        for item in data.get("list", []):
            stock_code = item.get("stock_code")
            corp_code = item.get("corp_code", "")
            rcept_no = item.get("rcept_no")
            corp_name = item.get("corp_name", "Unknown")
            rcept_dt = item.get("rcept_dt", "")  # âœ… ì ‘ìˆ˜ì¼ì ì¶”ì¶œ (ì˜ˆ: "20260119")

            # âœ… corp_codeê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (DB constraint ìœ„ë°˜ ë°©ì§€)
            if not corp_code or corp_code.strip() == "":
                logger.warning(f"â­ï¸ corp_code ì—†ìŒ - ê±´ë„ˆëœ€: {corp_name} (rcept_no: {rcept_no})")
                skipped += 1
                continue

            # âœ… ì¢…ëª©ì½”ë“œê°€ ì—†ê±°ë‚˜ ê³µë°±ì´ë©´ ê±´ë„ˆë›°ê¸°
            if not stock_code or stock_code.strip() == "" or stock_code == " ":
                logger.debug(f"â­ï¸ ì¢…ëª©ì½”ë“œ ì—†ìŒ - ê±´ë„ˆëœ€: {corp_name}")
                skipped += 1
                continue

            # âœ… rcept_dtê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš© (NOT NULL ì œì•½ ì¡°ê±´ ëŒ€ì‘)
            if not rcept_dt or rcept_dt.strip() == "":
                rcept_dt = datetime.now().strftime('%Y%m%d')
                logger.warning(f"âš ï¸ rcept_dt ì—†ìŒ - ì˜¤ëŠ˜ ë‚ ì§œë¡œ ëŒ€ì²´: {corp_name} (rcept_no: {rcept_no})")

            # âœ… ì¢…ëª©ì½”ë“œ, íšŒì‚¬ì½”ë“œ, ì ‘ìˆ˜ì¼ì ì •ë¦¬ (ê³µë°± ì œê±°)
            stock_code = stock_code.strip()
            corp_code = corp_code.strip()
            rcept_dt = rcept_dt.strip()

            # âœ… ì¤‘ë³µ ì²´í¬ (disclosure_hashes í…Œì´ë¸”)
            if is_disclosure_processed(corp_code, rcept_no):
                logger.debug(f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ë¨ - ê±´ë„ˆëœ€: {corp_name} ({rcept_no})")
                duplicates += 1
                continue

            payload = {
                "rcept_no": rcept_no,
                "corp_code": corp_code,
                "corp_name": corp_name,
                "stock_code": stock_code,
                "rcept_dt": rcept_dt,  # âœ… ì ‘ìˆ˜ì¼ì ì¶”ê°€
                "report_nm": item.get("report_nm"),
                "analysis_status": "pending",  # ë¶„ì„ ëŒ€ê¸° ìƒíƒœ
                "created_at": datetime.now().isoformat()
            }

            try:
                # disclosure_insights ì €ì¥
                supabase.table("disclosure_insights").upsert(payload, on_conflict="rcept_no").execute()

                # âœ… disclosure_hashesì— hash ì €ì¥
                hash_key = generate_hash_key(corp_code, rcept_no)
                hash_payload = {
                    "hash_key": hash_key,
                    "corp_code": corp_code,
                    "rcept_no": rcept_no,
                    "corp_name": corp_name,
                    "report_name": item.get("report_nm"),
                    "groq_analyzed": False,  # ì•„ì§ ë¶„ì„ ì „
                    "sonnet_analyzed": False,
                    "created_at": datetime.now().isoformat(),
                    "expires_at": (datetime.now() + timedelta(days=30)).isoformat()
                }

                supabase.table("disclosure_hashes").upsert(hash_payload, on_conflict="hash_key").execute()

                count += 1
                logger.info(f"âœ… [{count}] {corp_name} ({stock_code}) - {item.get('report_nm')[:40]}...")
            except Exception as e:
                logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")

        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ")
        logger.info(f"   - ì €ì¥: {count}ê±´")
        logger.info(f"   - ê±´ë„ˆëœ€ (corp_code/ì¢…ëª©ì½”ë“œ ì—†ìŒ): {skipped}ê±´")
        logger.info(f"   - ê±´ë„ˆëœ€ (ì¤‘ë³µ): {duplicates}ê±´")
        logger.info(f"   - ì´: {count + skipped + duplicates}ê±´")
        logger.info(f"{'='*70}\n")
    else:
        logger.warning(f"âš ï¸ DART API ì‘ë‹µ ì˜¤ë¥˜: {data.get('message', 'Unknown error')}")

if __name__ == "__main__":
    run_crawler()