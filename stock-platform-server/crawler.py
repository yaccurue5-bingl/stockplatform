import os
import re
import datetime
import time
import requests
import json
from bs4 import BeautifulSoup
from groq import Groq 
import OpenDartReader
from supabase import create_client

# Í≥µÌè¨ ÌÉêÏöï ÏßÄÏàò Í≥ÑÏÇ∞ ÌååÏùº ÏûÑÌè¨Ìä∏
try:
    import fg_index_calc 
except ImportError:
    fg_index_calc = None

# ÌÜµÌï© Î∂ÑÏÑù Î∞è ÏÉÅÏÑ∏ Ï∂îÏ∂ú ÌîÑÎ°¨ÌîÑÌä∏
DISCLOSURE_PROMPT_TEMPLATE = """
# Role: Professional Financial Analyst
# Language: Respond ONLY in KOREAN. (Ï§ëÏöî: ÎãµÎ≥ÄÏóê Ïô∏Íµ≠Ïñ¥Î•º Ï†àÎåÄ ÏÑûÏßÄ ÎßàÏÑ∏Ïöî.)

# Task
Ï†úÍ≥µÎêú Í≥µÏãú ÏõêÎ¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÌÜµÌï© Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
ÌäπÌûà 'Í≥ÑÏïΩ ÏÉÅÎåÄÎ∞©'Í≥º 'Í≥ÑÏïΩ Í∏àÏï°'ÏùÑ ÌÖçÏä§Ìä∏ÏóêÏÑú Ï†ïÌôïÌûà Ï∞æÏïÑ Î¶¨Ïä§Ìä∏ÏóÖÌïòÏÑ∏Ïöî.

# Input Data
Company: {corp_name}
Disclosure Details:
{disclosure_details}

# Constraints
1. **Headline**: ÏòÅÏñ¥Î°ú Îêú ÌÜµÌï© Ìó§ÎìúÎùºÏù∏ ÏûëÏÑ±.
2. **Key Takeaways (KOREAN)**: 
   - Í∞Å Í≥ÑÏïΩÎ≥Ñ ÏÉÅÏÑ∏ Î¶¨Ïä§Ìä∏: "1. [ÏÉÅÎåÄÎ∞©] [Í∏àÏï°] Í≥ÑÏïΩ, [Ï†ïÏ†ï/ÌïµÏã¨ ÎÇ¥Ïö©]"
   - Ïã§Ï†ú ÌöåÏÇ¨Î™ÖÍ≥º Í∏àÏï°ÏùÑ Î™ÖÏãúÌïòÏÑ∏Ïöî.
3. **Sentiment**: -1.0 ~ 1.0 Ï†êÏàò.
4. **JSON Format**: Output strictly in JSON.
"""

IMPORTANT_KEYWORDS = ['Í≥µÍ∏âÍ≥ÑÏïΩ', 'Ïú†ÏÉÅÏ¶ùÏûê', 'Î¨¥ÏÉÅÏ¶ùÏûê', 'Ïã§Ï†ÅÎ∞úÌëú', 'Îã®ÏùºÌåêÎß§', 'Ïù∏Ïàò', 'Ìï©Î≥ë', 'M&A', 'ÏµúÎåÄÏ£ºÏ£ºÎ≥ÄÍ≤Ω']

DART_KEY = os.environ.get("DART_API_KEY")
GROQ_KEY = os.environ.get("GROQ_API_KEY")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

client = Groq(api_key=GROQ_KEY)
dart = OpenDartReader(DART_KEY)
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def get_market_indices():
    print("--- Fetching Market Indices ---")
    try:
        urls = {
            "KOSPI": "https://finance.naver.com/sise/sise_index.naver?code=KOSPI",
            "KOSDAQ": "https://finance.naver.com/sise/sise_index.naver?code=KOSDAQ",
            "USD/KRW": "https://finance.naver.com/marketindex/"
        }
        headers = {"User-Agent": "Mozilla/5.0"}
        results = {}
        for name, url in urls.items():
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            node = soup.select_one("#now_value") if name in ["KOSPI", "KOSDAQ"] else soup.select_one(".value")
            if node: results[name] = node.get_text(strip=True)

        for name, val in results.items():
            if val and val != "---":
                row = supabase.table("market_indices").select("history").eq("name", name).execute()
                
                # [ÏàòÏ†ï] NoneType Î∞è JSON ÌååÏã± ÏóêÎü¨ Î∞©Ïñ¥ Î°úÏßÅ
                raw_history = []
                if row.data and row.data[0].get('history') is not None:
                    raw_history = row.data[0]['history']
                
                if isinstance(raw_history, str):
                    hist = json.loads(raw_history)
                else:
                    hist = raw_history
                
                clean_val = float(val.replace(',', ''))
                hist.append(clean_val)
                if len(hist) > 10: hist = hist[-10:]
                
                supabase.table("market_indices").upsert({
                    "name": name, "current_val": val, "history": hist
                }, on_conflict="name").execute()
    except Exception as e:
        print(f"‚ùå Index Error: {e}")

def analyze_disclosure():
    print("=== K-Market Insight Data Pipeline Start ===")
    get_market_indices()
    if fg_index_calc: fg_index_calc.update_fear_greed_idx()
    
    today = datetime.datetime.now().strftime('%Y%m%d')
    try:
        df = dart.list(start=today, end=today)
    except Exception as e:
        print(f"‚ùå DART Error: {e}"); return

    if df is None or df.empty: return

    grouped = {}
    for _, row in df.iterrows():
        code = row.get('stock_code')
        if not code: continue
        if code not in grouped: grouped[code] = []
        grouped[code].append(row)

    for code, rows in grouped.items():
        targets = [r for r in rows if any(kw in r['report_nm'] for kw in IMPORTANT_KEYWORDS)]
        if not targets: continue

        corp_name = targets[0]['corp_name']
        rep_rcept_no = targets[0]['rcept_no']

        check = supabase.table("disclosure_insights").select("id").eq("rcept_no", rep_rcept_no).execute()
        if check.data: continue

        print(f"üéØ ÌÜµÌï© Î∂ÑÏÑù: {corp_name} ({len(targets)}Í±¥)")
        
        full_text = ""
        for t in targets:
            try:
                doc = dart.document(t['rcept_no'])
                clean = re.sub('<[^<]+?>', '', doc)
                full_text += f"\n[Ï†úÎ™©: {t['report_nm']}]\n{clean[:2000]}\n"
            except:
                full_text += f"\n[Ï†úÎ™©: {t['report_nm']}] ÏõêÎ¨∏ Ï∂îÏ∂ú Ïã§Ìå®\n"

        try:
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": "ÎÑàÎäî ÌïúÍµ≠ Í∏àÏúµ Ï†ÑÎ¨∏Í∞ÄÏïº. ÌïúÍµ≠Ïñ¥Î°úÎßå ÎãµÎ≥ÄÌï¥."},
                    {"role": "user", "content": DISCLOSURE_PROMPT_TEMPLATE.format(corp_name=corp_name, disclosure_details=full_text)}
                ],
                response_format={"type": "json_object"}
            )
            ai_res = json.loads(response.choices[0].message.content)
            
            report_title = targets[0]['report_nm'] + (f" Ïô∏ {len(targets)-1}Í±¥" if len(targets) > 1 else "")
            
            supabase.table("disclosure_insights").upsert({
                "corp_name": corp_name, "stock_code": code,
                "report_nm": report_title, "ai_summary": "\n".join(ai_res.get('summary', [])),
                "sentiment": "POSITIVE" if ai_res.get('sentiment_score', 0) > 0.1 else "NEUTRAL",
                "rcept_no": rep_rcept_no, "created_at": datetime.datetime.now().isoformat()
            }).execute()
        except Exception as e:
            print(f"‚ùå AI Error: {e}")

if __name__ == "__main__":
    analyze_disclosure()